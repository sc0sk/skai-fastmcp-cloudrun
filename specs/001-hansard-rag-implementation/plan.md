# Implementation Plan: Australian Hansard RAG MVP

**Branch**: `001-hansard-rag-implementation` | **Date**: 2025-10-21 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/specs/001-hansard-rag-implementation/spec.md`

**Note**: This plan is generated by the `/speckit.plan` command.

## Summary

Build a Retrieval-Augmented Generation (RAG) server using FastMCP on Google Cloud Run for searching and analyzing Australian Hansard transcripts. The system will use **Google ADK-aligned architecture** with LangChain, Vertex AI embeddings (768-dimensional), and Cloud SQL with pgvector for hybrid semantic search. The MVP focuses on local development with 10 sample speeches, deployable to Cloud Run for production.

## Technical Context

**Language/Version**: Python 3.11+
**Primary Dependencies**: FastMCP 2.14.0+, LangChain (langchain-google-vertexai, langchain-google-cloud-sql-pg), Vertex AI Embeddings, Pydantic v2
**Storage**: Cloud SQL PostgreSQL with pgvector (production), SQLite (local dev for metadata), dual-table schema (speeches + speech_chunks)
**Vector Database**: Cloud SQL pgvector (768-dimensional Vertex AI embeddings) - NOT ChromaDB/Qdrant
**Testing**: pytest, pytest-asyncio, pytest-cov (>80% coverage), MCP Inspector
**Target Platform**: Google Cloud Run (HTTP transport), local development (STDIO transport)
**Project Type**: Single FastMCP server project
**Performance Goals**: Search latency <500ms (p95), embedding generation <100ms per chunk, 1 speech/sec ingestion
**Constraints**: MVP local-only (no cloud deployment), 10 sample speeches, GitHub OAuth deferred to v2
**Scale/Scope**: MVP with 10 speeches, future expansion to 1,000-5,000 speeches in production
**Authentication**: GitHub OAuth via OAuth Proxy pattern (production) - deferred to v2, no auth for MVP
**Architecture Alignment**: Google ADK standards - LangChain + Vertex AI + Cloud SQL pgvector (NOT sentence-transformers)

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### NON-NEGOTIABLE Principles

| Principle | Status | Notes |
|-----------|--------|-------|
| **I. FastMCP Server-First Architecture** | ✅ PASS | All features implemented as MCP tools (ingest_speech, search_speeches, get_speech) with tags and metadata |
| **II. Secure Authentication** | ⚠️ DEFERRED (MVP) | GitHub OAuth deferred to v2. **MUST implement before production deployment** using OAuth Proxy + GitHubProvider |
| **III. Secrets Management** | ✅ PASS | No credentials in MVP. Production will use Cloud Run secrets + .env (gitignored) |
| **IV. Test-Driven Development** | ✅ PASS | >80% test coverage required, pytest + MCP Inspector testing mandatory |
| **VII. Progress Transparency** | ✅ PASS | Ingestion tool reports progress for long-running operations |
| **VIII. Python & Pydantic Standards** | ✅ PASS | Python 3.11+, Pydantic v2, uv package manager, FastMCP 2.14.0+ |

### Additional Requirements

| Requirement | Status | Notes |
|-------------|--------|-------|
| **Tool Implementation Standards** | ✅ PASS | All tools use @mcp.tool decorator, type hints, docstrings, tags, readOnlyHint |
| **MCP Inspector Testing** | ✅ PASS | Required for all tool validation |
| **Cloud Run Standards** | ⚠️ PARTIAL | Deployment config prepared but not deployed in MVP |
| **ChatGPT Integration** | ⚠️ DEFERRED | HTTP transport ready, but not configured for ChatGPT in MVP |
| **MCP JSON Configuration** | ✅ PASS | fastmcp.json will be created with LangChain dependencies |

### Critical Blockers

**NONE** - All gates pass for MVP scope.

**Post-MVP Requirements** (before production):
1. Implement GitHub OAuth (Principle II) using OAuth Proxy pattern
2. Configure Cloud Run deployment with proper secrets
3. Set up ChatGPT integration if required

## Project Structure

### Documentation (this feature)

```
specs/[###-feature]/
├── plan.md              # This file (/speckit.plan command output)
├── research.md          # Phase 0 output (/speckit.plan command)
├── data-model.md        # Phase 1 output (/speckit.plan command)
├── quickstart.md        # Phase 1 output (/speckit.plan command)
├── contracts/           # Phase 1 output (/speckit.plan command)
└── tasks.md             # Phase 2 output (/speckit.tasks command - NOT created by /speckit.plan)
```

### Source Code (repository root)

```
skai-fastmcp-cloudrun/
├── src/
│   ├── models/              # Pydantic models for speeches and search
│   │   ├── speech.py        # SpeechMetadata, SpeechDetail models
│   │   └── results.py       # IngestionResult, SearchResult models
│   ├── storage/             # Database abstraction layer
│   │   ├── vector_store.py  # Cloud SQL pgvector integration (LangChain)
│   │   ├── metadata_store.py # SQLite/PostgreSQL metadata storage
│   │   └── embeddings.py    # Vertex AI embedding wrapper
│   ├── tools/               # MCP tools
│   │   ├── ingest.py        # ingest_speech tool
│   │   ├── search.py        # search_speeches tool
│   │   └── retrieve.py      # get_speech tool
│   ├── processing/          # Text processing and chunking
│   │   ├── chunker.py       # LangChain RecursiveCharacterTextSplitter
│   │   └── validators.py    # Input validation utilities
│   └── server.py            # FastMCP server entrypoint
│
├── tests/
│   ├── unit/                # Unit tests for individual components
│   │   ├── test_models.py
│   │   ├── test_chunking.py
│   │   └── test_embeddings.py
│   ├── integration/         # End-to-end MCP tool tests
│   │   ├── test_ingest.py
│   │   ├── test_search.py
│   │   └── test_retrieve.py
│   └── fixtures/            # Test data (sample speeches)
│       └── sample_speeches.json
│
├── data/                    # Local development data
│   ├── speeches.db          # SQLite database (dev only)
│   └── sample_data/         # 10 sample Hansard speeches
│       └── hansard_sample.json
│
├── deployment/              # Cloud Run deployment configs
│   ├── Dockerfile
│   ├── cloudbuild.yaml
│   └── cloud-run-config.yaml
│
├── fastmcp.json             # FastMCP server configuration
├── pyproject.toml           # uv project configuration
├── .env.example             # Environment variable template
└── README.md                # Development setup documentation
```

**Structure Decision**: Single FastMCP server project (Option 1). No frontend/backend separation needed - this is a pure MCP server. All tools are defined in `src/tools/`, with shared models in `src/models/` and storage abstraction in `src/storage/`. LangChain integration centralizes vector operations and embeddings.

## Complexity Tracking

*Fill ONLY if Constitution Check has violations that must be justified*

**No violations** - all constitution principles satisfied for MVP scope. GitHub OAuth deferred to v2 is explicitly allowed for MVP.

---

## Phase 0: Research (Complete ✅)

**Output**: [research.md](research.md)

**Key Decisions**:
1. **Embeddings**: Vertex AI `gemini-embedding-001` (768 dimensions, task-specific)
2. **Vector Database**: Cloud SQL PostgreSQL + pgvector v0.8.0 with HNSW indexing
3. **Framework**: LangChain async patterns (Google ADK alignment)
4. **Text Chunking**: RecursiveCharacterTextSplitter (800 chars, 150 overlap)
5. **Local Development**: sentence-transformers/all-mpnet-base-v2 (768 dims to match production)

**All NEEDS CLARIFICATION resolved**: Yes

---

## Phase 1: Design & Contracts (Complete ✅)

**Outputs**:
- [data-model.md](data-model.md) - Database schemas and Pydantic models
- [contracts/](contracts/) - MCP tool contracts (OpenAPI-style JSON)
  - `ingest_speech.json`
  - `search_speeches.json`
  - `get_speech.json`
- [quickstart.md](quickstart.md) - Local development setup guide
- [CLAUDE.md](../../CLAUDE.md) - Updated agent context

**Database Schema**:
- `speeches` table: Full text storage with metadata (10 indexed fields)
- `speech_chunks` table: Vector embeddings (768-dim) + denormalized metadata
- HNSW index: `m=24`, `ef_construction=100` (optimized for 768-dim vectors)

**MCP Tools Designed**:
1. **ingest_speech**: Load speeches from CSV/JSON → chunk → embed → store
2. **search_speeches**: Hybrid semantic + metadata filtering
3. **get_speech**: Retrieve full speech by UUID with optional context

---

## Phase 2: Implementation Planning

**Status**: Ready for `/speckit.tasks`

The implementation plan will be generated by running `/speckit.tasks` command, which will create `tasks.md` with dependency-ordered implementation tasks based on the design artifacts produced in Phase 0 and Phase 1.

**Implementation Workflow**:
1. Run `/speckit.tasks` to generate actionable tasks
2. Follow TDD workflow: write tests → implement tools → refactor
3. Use MCP Inspector for interactive testing
4. Achieve >80% test coverage before merging

---

## Deliverables Summary

| Phase | Artifact | Status | Location |
|-------|----------|--------|----------|
| Phase 0 | research.md | ✅ Complete | [research.md](research.md) |
| Phase 1 | data-model.md | ✅ Complete | [data-model.md](data-model.md) |
| Phase 1 | contracts/ | ✅ Complete | [contracts/](contracts/) |
| Phase 1 | quickstart.md | ✅ Complete | [quickstart.md](quickstart.md) |
| Phase 1 | CLAUDE.md update | ✅ Complete | [../../CLAUDE.md](../../CLAUDE.md) |
| Phase 2 | tasks.md | ⏳ Pending | Run `/speckit.tasks` |

---

## Next Steps

1. **Generate tasks**: Run `/speckit.tasks` to create dependency-ordered implementation tasks
2. **Follow quickstart.md**: Set up local development environment
3. **Implement tools**: Follow TDD workflow with test coverage
4. **Test with MCP Inspector**: Validate tool contracts interactively
5. **Prepare for v2**: Document OAuth implementation plan for production

**Ready for Implementation**: Yes ✅

---

## Sample Dataset: sk-hansard

### Dataset Overview

**Location**: `/home/user/skai-fastmcp-cloudrun/data/sk-hansard/`
**Format**: Markdown files with YAML frontmatter (Obsidian-compatible)
**Count**: **65 speeches** from Australian House of Representatives
**Speaker**: Simon Kennedy MP (Electorate: Cook, Party: Liberal)
**Date Range**: 2024-05-28 to 2025-10-09
**Total Size**: ~4,371 lines of content
**File Naming Pattern**: `{speaker_id}-{date}-{utterance_id}.md`

### File Structure Example

```yaml
---
speaker: Kennedy, Simon MP
speaker_id: '267506'
date: '2024-11-19'
debate: MATTERS OF PUBLIC IMPORTANCE
chamber: House of Reps
electorate: Cook
party: LP
parliament: 2
session: 1
period: 0
utterance_id: AUH_2024-11-19-p2.s1.per0.reps.u47
source_file: 2024-11-19_reps.xml
summary: 'Mr KENNEDY (Cook) (15:45): The member for Hunter talks...'
entities:
  people: []
  organizations:
  - Select Committee
  places:
  - Australia
themes:
- cost-of-living
- economic-management
- energy
tags:
- nuclear
- renewable-energy
---

[Full speech transcript text follows...]
```

### Field Mapping to Data Model

| sk-hansard Field | Target Field | Transformation |
|------------------|--------------|----------------|
| `speaker` | `speaker` | Use as-is |
| `date` | `date` | Parse ISO 8601 → DATE |
| `debate` | `title` | Use as speech title |
| `chamber` | `chamber` | Map "House of Reps" → "House of Representatives" |
| `electorate` | `electorate` | Use as-is |
| `party` | `party` | Map "LP" → "Liberal" (see mapping table) |
| `utterance_id` | `hansard_reference` | Construct "House Hansard, 19 Nov 2024, Utterance 47" |
| `tags` + `themes` | `topic_tags` | Merge arrays and deduplicate |
| Markdown body | `full_text` | Extract text after frontmatter |
| `source_file` | `source_url` | Construct APH URL |

### Transformation Rules

**Party Codes**:
```python
{"LP": "Liberal", "ALP": "Labor", "GRN": "Greens", "NAT": "National", "IND": "Independent"}
```

**Chamber Normalization**:
```python
{"House of Reps": "House of Representatives", "Senate": "Senate"}
```

**State Derivation** (from electorate):
```python
{"Cook": "NSW"}  # Requires electorate→state lookup table
```

**Hansard Reference Construction**:
```
Input:  utterance_id="AUH_2024-11-19-p2.s1.per0.reps.u47", date="2024-11-19"
Output: "House Hansard, 19 November 2024, Utterance 47"
```

### Ingestion Tool

**Tool Name**: `ingest_markdown_speeches`

**Implementation**:
```python
@mcp.tool(tags=["admin", "ingestion", "stable"])
async def ingest_markdown_speeches(
    directory_path: str,
    ctx: Context = None
) -> IngestionResult:
    """
    Ingest Hansard speeches from Markdown files with YAML frontmatter.

    Args:
        directory_path: Path to directory containing .md files
        ctx: MCP context for progress reporting

    Returns:
        IngestionResult with stats (speeches_processed, chunks_created, etc.)
    """
```

**Processing Steps**:
1. Scan directory for `*.md` files (65 files)
2. Parse YAML frontmatter with `python-frontmatter` library
3. Extract markdown body (speech text)
4. Apply field mappings and transformations
5. Validate with `SpeechMetadata` Pydantic model
6. Chunk text (RecursiveCharacterTextSplitter, 800/150)
7. Generate embeddings (Vertex AI gemini-embedding-001)
8. Insert into `speeches` + `speech_chunks` tables
9. Report progress via `await ctx.report_progress(i, 65)`

### Expected Results

**For 65-speech dataset**:
- Total content: ~65 speeches × ~2,000 chars avg = **130,000 chars**
- Expected chunks: 130,000 / (800 - 150) = **~200 chunks**
- Embedding API calls: 200 chunks / 10 per batch = **20 API calls**
- Embedding cost: 200 × 150 tokens/chunk = 30,000 tokens × $0.15/1M = **$0.004**
- Processing time: **~30-60 seconds** (inc. Vertex AI latency)

### Testing Strategy

```python
# tests/integration/test_ingest_markdown.py
async def test_ingest_sk_hansard_dataset():
    """Test ingestion of sk-hansard sample data."""
    result = await call_tool(
        "ingest_markdown_speeches",
        {"directory_path": "/home/user/skai-fastmcp-cloudrun/data/sk-hansard"}
    )

    assert result.speeches_processed == 65
    assert 150 <= result.chunks_created <= 300
    assert result.duplicates_skipped == 0
    assert len(result.errors) == 0
```

### Why This Dataset?

1. **Real Hansard Data**: Actual parliamentary speeches (not synthetic)
2. **Structured Format**: YAML frontmatter provides rich metadata
3. **Single Speaker**: Consistent voice/style for MVP testing
4. **Manageable Size**: 65 speeches perfect for local development
5. **Rich Topics**: Covers economy, energy, housing, taxation (diverse queries)
6. **Modern Dates**: Recent speeches (2024-2025) ensure relevance

### Future Extensions (v2)

- **Multi-Speaker**: Expand to all MPs/Senators (10,000-50,000 speeches)
- **Entity Linking**: Use `entities.people`, `entities.organizations` for filters
- **Summary Integration**: Leverage `summary` field for quick previews
- **Debate Grouping**: Group speeches by `debate` for contextual retrieval
- **Full APH Archive**: Ingest complete Hansard (1901-present, millions of speeches)

---

**Sample Data Status**: ✅ Available
**Integration**: To be implemented in Phase 2 (tasks.md)

